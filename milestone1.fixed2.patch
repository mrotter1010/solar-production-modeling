diff --git a/run.py b/run.py 
new file mode 100644
index 0000000..2a97b90
--- /dev/null
+++ b/run.py
@@ -0,0 +1,5 @@
+from solar_production_modeling.cli import main
+
+
+if __name__ == "__main__":
+ raise SystemExit(main())
diff --git a/scripts/verify.sh b/scripts/verify.sh 
new file mode 100755
index 0000000..7b4e189
--- /dev/null
+++ b/scripts/verify.sh
@@ -0,0 +1,37 @@
+#!/usr/bin/env bash
+set
-euo pipefail
+
+tmp_dir=$(mktemp
-d)
+outputs_root="$tmp_dir/outputs"
+
+python run.py
--input tests/data/sample_sites.csv
--outputs-root "$outputs_root"
--log-level INFO
+
+run_dir=$(ls "$outputs_root/_runs")
+run_inputs="$outputs_root/_runs/$run_dir/run_inputs_${run_dir}.json"
+
+if [[ !
-f "$run_inputs" ]]; then
+ echo "Missing run inputs file: $run_inputs" >&2
+ exit 1
+fi
+
+customer_dir="$outputs_root/Acme_Energy"
+site_dir="$customer_dir/Desert_Site/$run_dir"
+if [[ !
-d "$site_dir" ]]; then
+ echo "Missing site output directory: $site_dir" >&2
+ exit 1
+fi
+
+inputs_file="$site_dir/Desert_Site_Run_A_${run_dir}_inputs.json"
+if [[ !
-f "$inputs_file" ]]; then
+ echo "Missing site inputs file: $inputs_file" >&2
+ exit 1
+fi
+
+invalid_site_dir="$customer_dir/unknown/$run_dir"
+error_file="$invalid_site_dir/unknown_Run_B_${run_dir}_error.json"
+if [[ !
-f "$error_file" ]]; then
+ echo "Missing error file: $error_file" >&2
+ exit 1
+fi
+
+pytest
-q
diff --git a/solar_production_modeling/__init__.py b/solar_production_modeling/__init__.py 
new file mode 100644
index 0000000..e907c58
--- /dev/null
+++ b/solar_production_modeling/__init__.py
@@ -0,0 +1 @@
+__all__ = ["cli", "runner", "io", "util"]
diff --git a/solar_production_modeling/cli.py b/solar_production_modeling/cli.py 
new file mode 100644
index 0000000..9de2c40
--- /dev/null
+++ b/solar_production_modeling/cli.py
@@ -0,0 +1,50 @@
+from __future__ import annotations
+
+import argparse
+import logging
+import sys
+
+from solar_production_modeling.runner import run_batch
+
+
+def build_parser()
-> argparse.ArgumentParser:
+ parser = argparse.ArgumentParser(description="Solar production batch runner")
+ parser.add_argument("--input", required=True, help="Path to input CSV")
+ parser.add_argument("--outputs-root", default="outputs", help="Outputs root directory")
+ parser.add_argument("--fail-fast", action="store_true", help="Stop on first row error")
+ parser.add_argument(
+ "--log-level",
+ default="INFO",
+ choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
+ help="Logging verbosity",
+ )
+ return parser
+
+
+def main(argv: list[str] | None = None)
-> int:
+ parser = build_parser()
+ args = parser.parse_args(argv)
+
+ logging.basicConfig(level=getattr(logging, args.log_level))
+ logger = logging.getLogger("solar_production_modeling")
+
+ try:
+ result = run_batch(
+ input_csv=args.input,
+ outputs_root=args.outputs_root,
+ fail_fast=args.fail_fast,
+ logger=logger,
+ )
+ except Exception as exc:
+ logger.error("Failed to run batch: %s", exc)
+ return 1
+
+ if result.success_count == 0:
+ logger.error("All rows failed")
+ return 1
+
+ return 0
+
+
+if __name__ == "__main__":
+ sys.exit(main())
diff --git a/solar_production_modeling/io.py b/solar_production_modeling/io.py 
new file mode 100644
index 0000000..cb43b28
--- /dev/null
+++ b/solar_production_modeling/io.py
@@ -0,0 +1,44 @@
+from __future__ import annotations
+
+import csv
+import json
+from pathlib import Path
+from typing import Iterable
+
+
+def read_csv_rows(path: str | Path)
-> list[dict[str, str]]:
+ csv_path = Path(path)
+ with csv_path.open(newline="", encoding="utf-8") as handle:
+ reader = csv.DictReader(handle)
+ if reader.fieldnames is None:
+ raise ValueError("CSV is missing header row")
+ fieldnames = [name.strip() for name in reader.fieldnames]
+ if len(set(fieldnames)) != len(fieldnames):
+ raise ValueError("CSV header has duplicate column names after trimming")
+ rows: list[dict[str, str]] = []
+ for row in reader:
+ normalized = {key.strip(): (value if value is not None else "") for key, value in row.items()}
+ rows.append(normalized)
+ return rows
+
+
+def write_json(path: str | Path, data: dict | list)
-> None:
+ output_path = Path(path)
+ output_path.parent.mkdir(parents=True, exist_ok=True)
+ with output_path.open("w", encoding="utf-8") as handle:
+ json.dump(data, handle, indent=2, sort_keys=True)
+ handle.write("\n")
+
+
+def ensure_directory(path: str | Path)
-> Path:
+ output_path = Path(path)
+ output_path.mkdir(parents=True, exist_ok=True)
+ return output_path
+
+
+def write_text(path: str | Path, lines: Iterable[str])
-> None:
+ output_path = Path(path)
+ output_path.parent.mkdir(parents=True, exist_ok=True)
+ with output_path.open("w", encoding="utf-8") as handle:
+ for line in lines:
+ handle.write(f"{line}\n")
diff --git a/solar_production_modeling/runner.py b/solar_production_modeling/runner.py 
new file mode 100644
index 0000000..81eb8fd
--- /dev/null
+++ b/solar_production_modeling/runner.py
@@ -0,0 +1,137 @@
+from __future__ import annotations
+
+import logging
+from dataclasses import dataclass
+from pathlib import Path
+
+from solar_production_modeling import io, util
+
+REQUIRED_IDENTIFIERS = ["Customer", "Site Name", "Run Name"]
+
+
+@dataclass
+class RunResult:
+ run_datetime: str
+ output_root: Path
+ run_inputs_path: Path
+ success_count: int
+ error_count: int
+
+
+def _validate_required_columns(rows: list[dict[str, str]])
-> None:
+ if not rows:
+ raise ValueError("CSV contains no data rows")
+ available = set(rows[0].keys())
+ missing = [col for col in REQUIRED_IDENTIFIERS if col not in available]
+ if missing:
+ raise ValueError(f"CSV missing required columns: {', '.join(missing)}")
+
+
+def run_batch(
+ input_csv: str | Path,
+ outputs_root: str | Path = "outputs",
+ fail_fast: bool = False,
+ logger: logging.Logger | None = None,
+)
-> RunResult:
+ log = logger or logging.getLogger(__name__)
+ rows = io.read_csv_rows(input_csv)
+ _validate_required_columns(rows)
+
+ run_datetime = util.current_run_timestamp()
+ output_root = Path(outputs_root)
+ run_records: list[dict[str, str]] = []
+ success_count = 0
+ error_count = 0
+
+ stop_processing = False
+
+ for index, row in enumerate(rows, start=1):
+ row_record = dict(row)
+ row_record["__row_index"] = str(index)
+ row_record["__status"] = "ok"
+
+ customer = row.get("Customer", "")
+ site_name = row.get("Site Name", "")
+ run_name = row.get("Run Name", "")
+
+ customer_segment = util.sanitize_segment(customer)
+ site_segment = util.sanitize_segment(site_name)
+ run_segment = util.sanitize_segment(run_name)
+
+ site_dir = output_root / customer_segment / site_segment / run_datetime
+
+ error_info: dict[str, str] | None = None
+ try:
+ io.ensure_directory(site_dir)
+
+ input_filename = f"{site_segment}_{run_segment}_{run_datetime}_inputs.json"
+ input_path = site_dir / input_filename
+ io.write_json(input_path, row)
+
+ missing_identifiers = [
+ name for name in REQUIRED_IDENTIFIERS if not (row.get(name) or "").strip()
+ ]
+ if missing_identifiers:
+ error_info = {
+ "error_type": "ValidationError",
+ "message": f"Missing required identifiers: {', '.join(missing_identifiers)}",
+ }
+ except Exception as exc:
+ error_info = {
+ "error_type": "UnexpectedError",
+ "message": str(exc),
+ }
+
+ if error_info:
+ error_count
+= 1
+ row_record["__status"] = "error"
+ row_record["__error_type"] = error_info["error_type"]
+ row_record["__error_message"] = error_info["message"]
+
+ error_payload = {
+ "error_type": error_info["error_type"],
+ "message": error_info["message"],
+ "row_index": index,
+ "identifiers": {
+ "Customer": customer,
+ "Site Name": site_name,
+ "Run Name": run_name,
+ },
+ }
+ error_filename = f"{site_segment}_{run_segment}_{run_datetime}_error.json"
+ error_path = site_dir / error_filename
+ try:
+ io.write_json(error_path, error_payload)
+ except Exception as exc:
+ log.error("Failed to write error file for row %s: %s", index, exc)
+
+ log.error("Row %s failed: %s", index, error_info["message"])
+ if fail_fast:
+ stop_processing = True
+ else:
+ success_count
+= 1
+ log.info("Row %s processed successfully", index)
+
+ run_records.append(row_record)
+
+ if stop_processing:
+ break
+
+ run_inputs_dir = output_root / "_runs" / run_datetime
+ run_inputs_path = run_inputs_dir / f"run_inputs_{run_datetime}.json"
+ io.write_json(
+ run_inputs_path,
+ {
+ "run_datetime": run_datetime,
+ "input_csv_path": str(Path(input_csv)),
+ "rows": run_records,
+ },
+ )
+
+ return RunResult(
+ run_datetime=run_datetime,
+ output_root=output_root,
+ run_inputs_path=run_inputs_path,
+ success_count=success_count,
+ error_count=error_count,
+ )
diff --git a/solar_production_modeling/util.py b/solar_production_modeling/util.py 
new file mode 100644
index 0000000..4525033
--- /dev/null
+++ b/solar_production_modeling/util.py
@@ -0,0 +1,20 @@
+from __future__ import annotations
+
+from datetime import datetime
+
+_ALLOWED_CHARS = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789._-")
+_DEFAULT_SEGMENT = "unknown"
+
+
+def current_run_timestamp()
-> str:
+ return datetime.now().strftime("%Y-%m-%dT%H-%M-%S")
+
+
+def sanitize_segment(value: str | None, default: str = _DEFAULT_SEGMENT)
-> str:
+ if value is None:
+ value = ""
+ stripped = value.strip()
+ if not stripped:
+ return default
+ sanitized = "".join(ch if ch in _ALLOWED_CHARS else "_" for ch in stripped)
+ return sanitized or default
diff --git a/tests/data/sample_sites.csv b/tests/data/sample_sites.csv 
new file mode 100644
index 0000000..e19d0c9
--- /dev/null
+++ b/tests/data/sample_sites.csv
@@ -0,0 +1,4 @@
+Customer,Site Name,Run Name,Latitude,Longitude,DC Size (MW),AC Installed (MW),AC POI (MW),Racking,Tilt,Azimuth,Ground Clearance Height (m),Panel Model,Bifacial,Inverter Model,Shading (%),DC Wiring Loss (%),AC Wiring Loss (%),Transformer Losses (%),Degradation (%),Availability (%),Module Mismatch (%),LID(%),BESS Dispatch Required,BESS Optimization Required
+Acme Energy,Desert Site,Run A,35.1,-115.2,100,90,85,Fixed,25,180,1.2,Panel X,Yes,Inverter A,1,2,1,0.5,0.5,99,1,1,No,No
+Acme Energy,,Run B,36.0,-116.0,50,45,40,Fixed,20,180,1.0,Panel Y,No,Inverter B,1,2,1,0.5,0.5,99,1,1,No,No
+Beta Power,Coastal Site,Run C,33.9,-118.4,75,70,65,Tracker,10,180,0.8,Panel Z,Yes,Inverter C,1,2,1,0.5,0.5,99,1,1,Yes,No
diff --git a/tests/test_smoke.py b/tests/test_smoke.py 
new file mode 100644
index 0000000..3931868
--- /dev/null
+++ b/tests/test_smoke.py
@@ -0,0 +1,49 @@
+from __future__ import annotations
+
+from pathlib import Path
+import sys
+
+PROJECT_ROOT = Path(__file__).resolve().parents[1]
+sys.path.insert(0, str(PROJECT_ROOT))
+
+from solar_production_modeling import runner, util
+
+
+def test_smoke(tmp_path: Path)
-> None:
+ sample_csv = Path("tests/data/sample_sites.csv")
+ outputs_root = tmp_path / "outputs"
+
+ result = runner.run_batch(sample_csv, outputs_root=outputs_root)
+
+ run_datetime = result.run_datetime
+ run_inputs_path = outputs_root / "_runs" / run_datetime / f"run_inputs_{run_datetime}.json"
+ assert run_inputs_path.exists()
+
+ customer = util.sanitize_segment("Acme Energy")
+ site_valid = util.sanitize_segment("Desert Site")
+ run_name = util.sanitize_segment("Run A")
+ valid_site_dir = outputs_root / customer / site_valid / run_datetime
+ assert valid_site_dir.exists()
+
+ valid_inputs = valid_site_dir / f"{site_valid}_{run_name}_{run_datetime}_inputs.json"
+ assert valid_inputs.exists()
+
+ invalid_site = util.sanitize_segment("")
+ invalid_run = util.sanitize_segment("Run B")
+ invalid_site_dir = outputs_root / customer / invalid_site / run_datetime
+ assert invalid_site_dir.exists()
+
+ invalid_error = invalid_site_dir / f"{invalid_site}_{invalid_run}_{run_datetime}_error.json"
+ assert invalid_error.exists()
+
+ other_customer = util.sanitize_segment("Beta Power")
+ other_site = util.sanitize_segment("Coastal Site")
+ other_run = util.sanitize_segment("Run C")
+ other_site_dir = outputs_root / other_customer / other_site / run_datetime
+ assert other_site_dir.exists()
+
+ other_inputs = other_site_dir / f"{other_site}_{other_run}_{run_datetime}_inputs.json"
+ assert other_inputs.exists()
+
+ assert result.success_count == 2
+ assert result.error_count == 1
